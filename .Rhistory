install.packages(tidyverse)
install.packages('tidyverse')
# Install pacman ('package manager') if needed
if (!require('pacman')) install.packages('pacman')
# Get info on p_data() function from pacman package
?p_data()
# Get info on tidyverse datasets
p_data(tidyverse)
p_data(ggplot2)
# Import XML data from the web (must be online)
df <- 'http://ergast.com/api/f1/1954/results/1.xml' %>%
XML20bs() %>%
collapse_obs() %>% # saves the nested structure as a list, we'll convert to df
print()
library(tidyverse)
# Import XML data from the web (must be online)
df <- 'http://ergast.com/api/f1/1954/results/1.xml' %>%
XML20bs() %>%
collapse_obs() %>% # saves the nested structure as a list, we'll convert to df
print()
pacmad::p_load(XML2R)
pacman::p_load(XML2R)
# Import XML data from the web (must be online)
df <- 'http://ergast.com/api/f1/1954/results/1.xml' %>%
XML20bs() %>%
collapse_obs() %>% # saves the nested structure as a list, we'll convert to df
print()
# Import XML data from the web (must be online)
df <- 'http://ergast.com/api/f1/1954/results/1.xml' %>%
XML2Obs() %>%
collapse_obs() %>% # saves the nested structure as a list, we'll convert to df
print()
df %>% names() %>% print()
df %<>% # %<>% is the compound assignment pipe operator, performs function in place
as_tibble() %>%
select(
Race = matches('RaceName'),
FirstName = matches('GivenName'),
LastName = matches('FamilyName'),
Team = matches('Construcor//Name')
) %>%
print() # Each column is a list with two values (oops)
?par
?title
?par
?par()
source("~/tutorial.R", echo=TRUE)
?legend
?source
?caption
library(shiny); runApp('Documents/Coding/yield_curve/yield_curve.R')
return(feds200628)
runApp('Documents/Coding/yield_curve/yield_curve.R')
runApp('Documents/Coding/yield_curve/yield_curve.R')
url <- "https://www.federalreserve.gov/econres/feds/the-us-treasury-yield-curve-1961-to-the-present.htm"
# Read the webpage
page <- read_html(url)
library(httr)
library(rvest)
url <- "https://www.federalreserve.gov/econres/feds/the-us-treasury-yield-curve-1961-to-the-present.htm"
# Read the webpage
page <- read_html(url)
# Extract the link to the latest spreadsheet (find the download link in the page)
download_link <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
.[grepl("csv$", .)]  # Filter for csv files
# Full URL for the spreadsheet
download_url <- paste0("https://www.federalreserve.gov", download_link)
runApp('Documents/Coding/yield_curve/yield_curve.R')
# Define the destination path (you can add a timestamp to the file name if desired)
dest_file <- file.path("feds200628.csv")
# Download the file and save it
GET(download_url, write_disk(dest_file, overwrite = TRUE))
message("File downloaded successfully and saved at: ", dest_file)
download_data()
# This function downloads the most recent GSW yield data every Tuesday at 5pm
download_data <- function() {
url <- "https://www.federalreserve.gov/econres/feds/the-us-treasury-yield-curve-1961-to-the-present.htm"
# Read the webpage
page <- read_html(url)
# Extract the link to the latest spreadsheet (find the download link in the page)
download_link <- page %>%
html_nodes("a") %>%
html_attr("href") %>%
.[grepl("csv$", .)]  # Filter for csv files
# Full URL for the spreadsheet
download_url <- paste0("https://www.federalreserve.gov", download_link)
# Define the destination path (you can add a timestamp to the file name if desired)
dest_file <- file.path("feds200628.csv")
# Download the file and save it
GET(download_url, write_disk(dest_file, overwrite = TRUE))
message("File downloaded successfully and saved at: ", dest_file)
}
download_data()
runApp('Documents/Coding/yield_curve/yield_curve.R')
# Create the chart in Plotly
plot_ly(
data = plot_data,
x=~x,
y=~y, type = 'scatter',
mode='lines+markers'
) %>%
layout(
title= paste("Yield Curve on", format(selected_date, "%B %d, %Y")),
xaxis = list(title = "Horizon (Years)"),
yaxis = list(title = "Yield (Percent)",
range = c(0, 10),
autorange=FALSE)
)
runApp('Documents/Coding/yield_curve/yield_curve.R')
install.packages("IBrokers")
library(IBrokers)
tws <- twsConnect(clientId = 1, host = "127.0.0.1", port = 7496)
df <- read_data()
df <- convert_to_daily(df)
source("~/Documents/Research/PredictionMarkets/code/ffr_distributions_timeseries.R", echo=TRUE)
fomc_dates <- df %>% unique(expiry_date)
fomc_dates <- df %>% unique(expiry_dates)
View(df)
fomc_dates <- df %>% unique(expiry_date)
fomc_dates <- df %>% unique()
fomc_dates <- fomc_dates$expiry_date %>% unique()
fomc_dates
fomc_dates <- fomc_dates %>% arrange()
fomc_dates <- df$expiry_date %>% arrange %>% unique()
fomc_dates <- df$expiry_date %>% arrange() %>% unique()
source("~/Documents/Research/PredictionMarkets/code/ffr_distributions_timeseries.R", echo=TRUE)
source("~/Documents/Research/PredictionMarkets/code/ffr_distributions_timeseries.R", echo=TRUE)
source("~/Documents/Research/PredictionMarkets/code/cpi_distributions_timeseries.R", echo=TRUE)
source("~/Documents/Research/PredictionMarkets/code/cpi_distributions_timeseries.R", echo=TRUE)
View(moments_df)
source("~/Documents/Research/PredictionMarkets/code/cpi_distributions_timeseries.R", echo=TRUE)
